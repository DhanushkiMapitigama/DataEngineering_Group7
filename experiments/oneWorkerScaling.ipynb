{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "timing = []\n",
    "core = (1, 2, 3, 4, 5, 6, 7, 8)\n",
    "\n",
    "for x in core:\n",
    "    spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.122:7077\") \\\n",
    "        .appName(f\"Timing 1 workers {x} cores\")\\\n",
    "        .config(\"spark.cores.max\", f\"{x}\")\\\n",
    "        .config(\"spark.executor.memory\", \"2g\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    sc = SQLContext(spark_session.sparkContext)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    df = spark_session.read.json(\"hdfs://130.238.28.211:9000/reddit/RC_2011-08.json\")\n",
    "    df.createOrReplaceTempView('park')\n",
    "    df = sc.sql(\"select body,created_utc,subreddit from park\").where(df[\"body\"]!=\"[deleted]\").withColumn(\"hour\",hour(from_unixtime(col(\"created_utc\")))).drop('created_utc')\n",
    "\n",
    "    end = time.time()\n",
    "    timing.append(end-start)\n",
    "    print(f\"cores={x}, elapsed_time={end-start:.3f} seconds\")\n",
    "    spark_session.stop()\n",
    "\n",
    "### Plotting the results ###\n",
    "plt.scatter(core, timing)\n",
    "\n",
    "# Adding the perfect scaling line\n",
    "one_core_time = timing[0]\n",
    "perfect_scaling = [one_core_time / c for c in core]\n",
    "plt.plot(core, perfect_scaling, linestyle='dashed', label='Perfect Scaling')\n",
    "\n",
    "plt.xlabel('Cores')\n",
    "plt.ylabel('Elapsed Time (seconds)')\n",
    "plt.legend()\n",
    "plt.savefig(\"new Timing Cores 1-8 For 1 Workers\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
